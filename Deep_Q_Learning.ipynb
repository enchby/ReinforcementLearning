{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Deep_Q_Learning.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"_xn5D9yIIntr","colab_type":"code","colab":{}},"source":["!pip install gym"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wI7Lm6YtIEOv","colab_type":"code","colab":{}},"source":["import gym\n","import numpy as np\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.optimizers import Adam\n","from collections import deque\n","\n","# Create the Cart-Pole game environment\n","env = gym.make('CartPole-v0')\n","\n","\n","class QNetwork:\n","    def __init__(self, learning_rate=0.01, state_size=4,\n","                 action_size=2, hidden_size=10):\n","        # state inputs to the Q-network\n","        self.model = Sequential()\n","\n","        self.model.add(Dense(hidden_size, activation='relu',\n","                             input_dim=state_size))\n","        self.model.add(Dense(hidden_size, activation='relu'))\n","        self.model.add(Dense(action_size, activation='linear'))\n","\n","        self.model.compile(loss='mse', optimizer=Adam(lr=learning_rate))\n","\n","\n","class Memory():\n","    def __init__(self, max_size=1000):\n","        #dequeはmaxlenを記憶する箱で、maxlenを超えると古いものから押し出されていく\n","        self.buffer = deque(maxlen=max_size)\n","\n","    def add(self, experience):\n","        self.buffer.append(experience)\n","\n","    def sample(self, batch_size):\n","        idx = np.random.choice(np.arange(len(self.buffer)),\n","                               size=batch_size,\n","                               replace=False)\n","        return [self.buffer[ii] for ii in idx]\n","\n","\n","train_episodes = 20#1000          # max number of episodes to learn from\n","max_steps = 200#200                # max steps in an episode\n","gamma = 0.99                   # future reward discount\n","\n","# Exploration parameters\n","explore_start = 1.0            # exploration probability at start\n","explore_stop = 0.01            # minimum exploration probability\n","decay_rate = 0.0001            # exponential decay rate for exploration prob\n","\n","# Network parameters\n","hidden_size = 16               # number of units in each Q-network hidden layer\n","learning_rate = 0.001         # Q-network learning rate\n","\n","# Memory parameters\n","memory_size = 10000            # memory capacity\n","batch_size = 32                # experience mini-batch size\n","pretrain_length = batch_size   # number experiences to pretrain the memory\n","\n","mainQN = QNetwork(hidden_size=hidden_size, learning_rate=learning_rate)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LNJS4VkOPtIp","colab_type":"code","outputId":"7819437d-fe4e-4bd8-b593-93207ff6d671","executionInfo":{"status":"ok","timestamp":1565182510549,"user_tz":-540,"elapsed":1342,"user":{"displayName":"HIROYUKI ENDO","photoUrl":"https://lh6.googleusercontent.com/-v08tPTztbvc/AAAAAAAAAAI/AAAAAAAACJ4/qY0lm2ab5Pk/s64/photo.jpg","userId":"05258965053732855631"}},"colab":{"base_uri":"https://localhost:8080/","height":252}},"source":["mainQN.model.summary()"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense (Dense)                (None, 16)                80        \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 16)                272       \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 2)                 34        \n","=================================================================\n","Total params: 386\n","Trainable params: 386\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vgaGwjyGI4cI","colab_type":"code","colab":{}},"source":["###################################\n","## Populate the experience memory\n","###################################\n","\n","# Initialize the simulation\n","#現時点での状態を表す為の初期化\n","#初期化のたびに異なるstateを作っている\n","env.reset()\n","#env.action_space.sample()は01のどちらかをランダムに出す\n","state, reward, done, _ = env.step(env.action_space.sample())\n","#現状パラメーターstateを(1,4)のnumpy shapeにする\n","state = np.reshape(state, [1, 4])\n","#メモリーの初期化\n","memory = Memory(max_size=memory_size)\n","\n","\n","#env.step(action) で (array([-0.04418486,  0.34338878,  0.01652308, -0.52337281]), 1.0, False, {})　が返る"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"toDNsP1FJC-m","colab_type":"code","colab":{}},"source":["#############ここのループはおそらく32個(pretrain_length)の強化学習テーブルをつくる為のもの?##############\n","for ii in range(pretrain_length):\n","    # ヴィジュアル化させないためのuncomment\n","    # env.render()\n","\n","    #ランダムに0か1（右か左）を返す\n","    action = env.action_space.sample()\n","    #01(右左)の選択結果としての現状パラメーター、報酬、終了かどうかを返す\n","    #なぜ変数名がnext_stateになっているかというと、上段セルPopulate the experience memoryですでにstateができているので、その次を用意する為\n","    next_state, reward, done, _ = env.step(action)\n","    #現状パラメーターを(1,4)のnumpy shapeにする\n","    next_state = np.reshape(next_state, [1, 4])\n","\n","    if done:\n","        #失敗に終わったので次につながるパラメーター(1,4)を0で埋める\n","        next_state = np.zeros(state.shape)\n","        #今回の(現状パラメーター、右左、報酬-1、次につながるパラメーター)を記録\n","        #言いたいことは、この状態(state)で右左(01)のactionをしたら失敗しました。だから報酬は-1でnext_stateは0です\n","        memory.add((state, action, reward, next_state))\n","        #現在の状態をリセット\n","        env.reset()\n","        #新しい状態に初期化\n","        state, reward, done, _ = env.step(env.action_space.sample())\n","        #次のための現状パラメーターを用意\n","        state = np.reshape(state, [1, 4])\n","    else:\n","        #今回の(現状パラメーター、右左、報酬1、次につながるパラメーター)を記録\n","        #言いたいことは、この状態(state)で右左(01)のactionをしたら成功しました。だから報酬は1でnext_stateは繰越です\n","        memory.add((state, action, reward, next_state))\n","        #次のための現状パラメーターを用意\n","        state = next_state"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KibrlcfmJ98e","colab_type":"code","outputId":"41482d37-1119-4fab-a3c0-1ba550cb0de0","executionInfo":{"status":"ok","timestamp":1565182902371,"user_tz":-540,"elapsed":36918,"user":{"displayName":"HIROYUKI ENDO","photoUrl":"https://lh6.googleusercontent.com/-v08tPTztbvc/AAAAAAAAAAI/AAAAAAAACJ4/qY0lm2ab5Pk/s64/photo.jpg","userId":"05258965053732855631"}},"colab":{"base_uri":"https://localhost:8080/","height":336}},"source":["#############\n","## Training\n","#############\n","#ここのループで強化学習\n","#最大20ループ\n","step = 0\n","for ep in range(1, train_episodes):\n","    total_reward = 0\n","    #tはステップ数をカウントする為\n","    t = 0\n","    #1ループにつき最大200ステップ\n","    while t < max_steps:\n","        step += 1\n","        # Uncomment this next line to watch the training\n","        #env.render()\n","        \n","        ##################### 01のactionを決めるためのcode ####################\n","        #explore_pは1未満の数値になる。ループ回数ステップ回数増加とともにstep数値が大きくなるとexplore_pは0に近づいていく\n","        #つまりステップ回数が増えていくとランダム数値を選ぶ確率が低くなっていく\n","        explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*step)\n","        #np.random.rand()は0から1までのランダム数値(float)を返す\n","        #下記if statementは過学習防止のため、ランダム数値か学習modelのpredictかを振り分けてaction(01)を決めている\n","        if explore_p > np.random.rand():\n","            # ランダムでaction決定\n","            action = env.action_space.sample()\n","        else:\n","            # Q-networkによるaction決定\n","            Qs = mainQN.model.predict(state)[0]\n","            action = np.argmax(Qs)\n","        ##################### 01のactionを決めるためのcode end ####################\n","            \n","                                    #---#\n","            \n","        ##################### actionによる結果生成のcode ####################\n","        #新しい状況パラメーターや報酬などを生成\n","        next_state, reward, done, _ = env.step(action)\n","        next_state = np.reshape(next_state, [1, 4])\n","        total_reward += reward\n","        ##################### actionによる結果生成のcode end ####################\n","\n","                                    #---#\n","        \n","        ##################### 結果合否からの条件分岐によるstate生成code ####################\n","        if done:\n","            #失敗に終わったので次につながるパラメーターを0で埋める\n","            next_state = np.zeros(state.shape)\n","            #今回のステップを終了(while構文)させるため、上限の200をセット\n","            t = max_steps\n","\n","            print('Episode: {}'.format(ep),\n","                  'Total reward: {}'.format(total_reward),\n","                  'Explore P: {:.4f}'.format(explore_p))\n","\n","            #結果を記録\n","            memory.add((state, action, reward, next_state))\n","            #現在の状態をリセット\n","            env.reset()\n","            #新しい状態に初期化\n","            state, reward, done, _ = env.step(env.action_space.sample())\n","            state = np.reshape(state, [1, 4])\n","        else:\n","            #今回の(現状パラメーター、右左、報酬1、次につながるパラメーター)を記録\n","            memory.add((state, action, reward, next_state))\n","            #次のための現状パラメーターを用意\n","            state = next_state\n","            #ステップ回数を更新\n","            t += 1\n","        ##################### 結果合否からの条件分岐によるstate生成code end ####################\n","\n","                                    #---#\n","        \n","        ##################### modelの学習code ####################\n","        # Replay\n","        #説明変数の箱を用意\n","        inputs = np.zeros((batch_size, 4))\n","        #目的変数の箱を用意\n","        targets = np.zeros((batch_size, 2))\n","        #memory.sample(batch_size)で強化学習テーブルの呼び出し\n","        minibatch = memory.sample(batch_size)\n","        #このループで強化学習テーブルを更新\n","        for i, (state_b, action_b, reward_b, next_state_b) in enumerate(minibatch):\n","            #強化学習テーブルから説明変数を取り出してi番目に入れる\n","            inputs[i:i+1] = state_b\n","            target = reward_b\n","            #.all(axis=1)は全てが0以上(True)かを判定する。他に .any()もある\n","            #よって、下記の条件式はnext_state_bが全て0ではない、成功で終わった学習セットである意味\n","            if not (next_state_b == np.zeros(state_b.shape)).all(axis=1):\n","                #このpredictで出てくる数字は0から1の確率数値 exmple[[0.4325,0.1213]] \n","                #target_Q = mainQN.model.predict(next_state_b)[0]\n","                #np.argmax()は最大値のindexを返すが、np.amax()は最大値そのものを返す\n","                target = reward_b + gamma * np.amax(mainQN.model.predict(next_state_b)[0])\n","            targets[i] = mainQN.model.predict(state_b)\n","            targets[i][action_b] = target\n","            \n","        #このステップでの結果をmodelにfit(学習)\n","        #ここでの目的変数(targets)は、(32,2)のシェイプで、右左(01)の時の数値が格納されている\n","        #よって、predictされた際には2つの数値が返され、argmaxで大きい方を呼び出す。\n","        mainQN.model.fit(inputs, targets, epochs=10, verbose=0)\n","        ##################### modelの学習code end ####################"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Episode: 1 Total reward: 3.0 Explore P: 0.9997\n","Episode: 2 Total reward: 17.0 Explore P: 0.9980\n","Episode: 3 Total reward: 23.0 Explore P: 0.9958\n","Episode: 4 Total reward: 15.0 Explore P: 0.9943\n","Episode: 5 Total reward: 20.0 Explore P: 0.9923\n","Episode: 6 Total reward: 12.0 Explore P: 0.9911\n","Episode: 7 Total reward: 16.0 Explore P: 0.9896\n","Episode: 8 Total reward: 15.0 Explore P: 0.9881\n","Episode: 9 Total reward: 21.0 Explore P: 0.9860\n","Episode: 10 Total reward: 10.0 Explore P: 0.9851\n","Episode: 11 Total reward: 14.0 Explore P: 0.9837\n","Episode: 12 Total reward: 17.0 Explore P: 0.9820\n","Episode: 13 Total reward: 27.0 Explore P: 0.9794\n","Episode: 14 Total reward: 14.0 Explore P: 0.9781\n","Episode: 15 Total reward: 23.0 Explore P: 0.9758\n","Episode: 16 Total reward: 41.0 Explore P: 0.9719\n","Episode: 17 Total reward: 19.0 Explore P: 0.9701\n","Episode: 18 Total reward: 20.0 Explore P: 0.9682\n","Episode: 19 Total reward: 24.0 Explore P: 0.9659\n"],"name":"stdout"}]}]}